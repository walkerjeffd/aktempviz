# AKTEMP-VIZ Stream Temperature Data Visualization Tool

https://aktemp.uaa.alaska.edu/viz/

## About

AKTEMP-VIZ is a data visualization tool for exploring spatial and temporal water temperature dynamics in streams and rivers across Alaska. The project combines water temperature data from three data sources:

- [AKTEMP Database](https://aktemp.uaa.alaska.edu/) (University of Alaska Anchorage)
- [USGS National Water Information System (NWIS)](https://waterdata.usgs.gov/nwis)
- [National Park Service (NPS) Aquarius Web Portal](https://irma.nps.gov/aqwebportal)

Air temperature data for each water temperature station is obtained from [ERA5-Land](https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_LAND_HOURLY) via Google Earth Engine.

## Project Structure

- `public/` - Static assets and processed data files
- `r/` - R scripts and docker image for data processing pipeline
- `src/` - Vue.js frontend application source code

## Data Pipeline

The dataset shown in AKTEMP-VIZ is generated by an automated data pipeline using the [{targets}](https://books.ropensci.org/targets/) package for R with cloud storage on GCP. The pipeline is run in a Docker container and the results are uploaded to the S3 bucket where the website is hosted.

### Set Up

#### Data Directory

The data directory located at `r/data` is where the input and output data files are stored. The pipeline will write the results to this directory.

```bash
mkdir r/data
```

Within the `r/data/` directory, there are two subdirectories:

```bash
mkdir -p r/data/gis # for storing gis data (i.e. WBD)
mkdir -p r/data/output # for storing the pipeline output data
```

Within the `r/data/gis/` directory, download the WBD dataset file for HUC2=19. The easiest way to do this is to download the [WBD_19_HU2_GDB](https://prd-tnm.s3.amazonaws.com/index.html?prefix=StagedProducts/Hydrography/WBD/HU2/GDB/WBD_19_HU2_GDB.zip) file from the [USGS National Hydrography Dataset](https://prd-tnm.s3.amazonaws.com/index.html?prefix=StagedProducts/Hydrography/WBD/HU2/GDB/) and unzip it into the `r/data/gis/` directory. The data pipeline will expect to find the WBD file at `r/data/gis/WBD_19_HU2_GDB/WBD_19_HU2_GDB.gdb`.

#### Configuration

Before running the pipeline, set up the environment variables. These can be set using a `.env` file within the `r/` directory.

```sh
# r/.env

# AKTEMP PostgreSQL Database
AKTEMP_HOST=
AKTEMP_PORT=
AKTEMP_DBNAME=
AKTEMP_USER=
AKTEMP_PASSWORD=

# Amazon Web Services Credentials
AWS_REGION=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# AWS S3 Bucket For Website Hosting (this is where the output files are uploaded to)
AWS_S3_BUCKET=
AWS_S3_PREFIX=

# Google Cloud Storage
# GCS_AUTH_JSON contains the JSON key file in base64 encoding
# GCS_AUTH_JSON=$(cat service-account.json | base64)
GCS_AUTH_JSON=
# GCS_BUCKET is the bucket for storing pipeline-related files (cache, GEE tasks, targets)
GCS_BUCKET=

# USGS Water Data API Key
API_USGS_PAT=

# Python Path for Reticulate
RETICULATE_PYTHON=/usr/bin/python
```

#### Google Earth Engine Setup

The pipeline retrieves air temperature data from ERA5-Land via Google Earth Engine. A GEE service account is required:

1. Create a Google Earth Engine service account (see [GEE Service Account docs](https://developers.google.com/earth-engine/guides/service_account))
2. Assign roles: Storage Admin, Service Usage Consumer, Earth Engine Resource Viewer (Beta)
3. Download the JSON key file (`r/service-account.json`)
4. Convert JSON file to string using base64 encoding (`cat r/service-account.json | base64`) and save output to `GCS_AUTH_JSON` in `.env`

#### Run Pipeline

The pipeline can be run manually or automatically.

To run the pipeline manually:

```r
source("_targets.R")
targets::tar_make()
```

To run the pipeline automatically, use the `run.R` script.

```bash
Rscript run.R
```

### Docker Container

The pipeline can also be run within a Docker container for deployment to the cloud.

1. Build the Docker container:

```bash
cd r
docker build -t aktemp/aktempviz-data --platform linux/amd64 .
```

2. Set up required environment variables in the `.env.docker` file (see `.env` file above)

3. Run the data processing pipeline via docker:

```sh
# run pipeline
docker run --rm \
  --env-file .env.docker \
  --platform linux/amd64 \
  aktemp/aktempviz-data

# open shell in temporary container
docker run -it --rm \
  --platform linux/amd64 \
  --entrypoint /bin/bash \
  aktemp/aktempviz-data
```

The pipeline will:

1. Collect data from AKTEMP, USGS, and NPS sources
2. Process and combine the datasets
3. Fetch ERA5-Land air temperature data from Google Earth Engine
4. Merge water and air temperature data
5. Generate output files in the data directory
6. Upload results to S3

**Note**: The pipeline uses targets cloud storage (GCP), so metadata and objects are automatically uploaded to GCS at `gs://${GCS_BUCKET}/targets/meta/` and `gs://${GCS_BUCKET}/targets/objects/`.

### Cloud Deployment

#### Deploy Docker Image to ECR

Create new ECR repo

```yml
name: aktemp/aktempviz-data
```

Deploy image to ECR

```sh
aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 083481224424.dkr.ecr.us-west-2.amazonaws.com

docker tag aktemp/aktempviz-data:latest 083481224424.dkr.ecr.us-west-2.amazonaws.com/aktemp/aktempviz-data:latest

docker push 083481224424.dkr.ecr.us-west-2.amazonaws.com/aktemp/aktempviz-data:latest
```

#### Batch Job

AWS Batch can be used to run the pipeline on a schedule.

First, create a compute environment.

```yml
name: aktempviz
type: FARGATE
vcpu: 8 max
vpc: {vpc_id}
subnets:
  - {subnet_id_a} ({region}a, public)
  - {subnet_id_b} ({region}b, public)
sg: {sg_id} (default)
```

Then create a job queue.

```yml
name: aktempviz
type: FARGATE
compute env: aktempviz
```

Create an IAM execution role for the ECS task (job).

```yml
name: aktempviz-execution
service: Elastic Container Service Task
policy: AmazonECSTaskExecutionRolePolicy
```

Create an IAM role for the job that includes S3 access.

```yml
name: aktempviz-job
service: Elastic Container Service Task
policies:
  - AmazonECSTaskExecutionRolePolicy
  - AmazonS3FullAccess
```

Create a job definition that uses the `aktemp/aktempviz-data` Docker image. Add the environment variables including GEE service account credentials.

```yml
name: aktempviz-data
type: FARGATE
legacy: FALSE
job attempts: 1
execution role: aktempviz-execution
task role: aktempviz-job
container:
  name: aktempviz-data
  image: {account}.dkr.ecr.{region}.amazonaws.com/aktemp/aktempviz-data:latest
  cpu: 2
  memory: 4GB
  command: ["Rscript","run.R"]
  env:
    - AKTEMP_DBNAME=
    - AKTEMP_HOST=
    - AKTEMP_PORT=
    - AKTEMP_USER=
    - AKTEMP_PASSWORD=
    - AWS_S3_BUCKET=
    - AWS_S3_PREFIX=
    - GCS_AUTH_JSON=
    - GCS_BUCKET=
    - API_USGS_PAT=
    - RETICULATE_PYTHON=/usr/bin/python
```

#### Weekly Schedule

To run the pipeline weekly, create an EventBridge rule that triggers a Batch job on Sunday at noon AK.

First, create an EventBridge role:

```yml
name: aktempviz-scheduler
```

The role needs a trust relationship with EventBridge.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "",
            "Effect": "Allow",
            "Principal": {
                "Service": "scheduler.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

Then create an inline policy that allows the job to submit a Batch job.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "batch:SubmitJob"
            ],
            "Resource": "*"
        }
    ]
}
```

Finally, create an EventBridge rule that triggers the Batch job.

```yml
name: aktempviz-data
type: schedule
occurance: recurring
tz: America/Anchorage
cron: 0 12 ? * 1 * (sunday at noon AK)
target: Batch SubmitJob
  job def: aktempviz-data
  job name: aktempviz-data-weekly
  job queue: aktempviz
retry: off
```

## Frontend Application

The frontend application is built with Vue.js 3 and Vuetify and designed to be hosted on AWS S3.

1. Install dependencies:

```bash
npm install
```

2. Start development server:

```bash
npm run dev
```

3. Build and deploy the application:

```bash
npm run generate
npm run deploy
```

The data files are not deployed to S3 by this npm script because they are generated using the data pipeline instead.

## License

see [LICENSE](LICENSE)

## Contact

Jeff Walker (@walkerjeffd)  
[Walker Environmental Research, LLC](https://walkerenvres.com)  
<jeff@walkerenvres.com>  
