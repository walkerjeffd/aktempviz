# AKTEMPVIZ Stream Temperature Data Visualization Tool

https://aktemp.uaa.alaska.edu/viz/

## About

AKTEMPVIZ is a data visualization tool for exploring spatial and temporal water temperature dynamics in streams and rivers across Alaska. The project combines water temperature data from three data sources:

- [AKTEMP Database](https://aktemp.uaa.alaska.edu/) (University of Alaska Anchorage)
- [USGS National Water Information System (NWIS)](https://waterdata.usgs.gov/nwis)
- [National Park Service (NPS) Aquarius Web Portal](https://irma.nps.gov/aqwebportal)

Air temperature data for each water temperature station is obtained from [Daymet](https://daymet.ornl.gov/).

## Project Structure

- `public/` - Static assets and processed data files
- `public/data/` - Raw and intermediate data files (not tracked in git)
- `R/` - R scripts for data collection and processing
- `src/` - Vue.js frontend application source code

## Data Pipeline

The dataset shown in AKTEMPVIZ is generated by an automated data pipeline using the [{targets}](https://books.ropensci.org/targets/) package for R. The pipeline is run in a Docker container and the results are uploaded to the S3 bucket where the website is hosted.

### Set Up

#### Data Directory

The data directory is where the raw and intermediate data files are stored. The pipeline will write the results to this directory. Note that this directory can be located anywhere on your system, but within the `r/` directory is simplest.

```bash
mkdir r/data
```

Within the `r/data/` directory, you will need to create the following subdirectories:

```bash
mkdir -p r/data/_targets # targets store for the docker container
mkdir -p r/data/daymet # for storing daymet tiles
mkdir -p r/data/gis # for storing gis data (i.e. WBD)
mkdir -p r/data/output # for storing the pipeline output data
```

The daymet files will be automatically downloaded by the pipeline and saved to `r/data/daymet/`.

Within the `r/data/gis/` directory, you will need to download the WBD data. The easiest way to do this is to download the [WBD_19_HU2_GDB](https://prd-tnm.s3.amazonaws.com/index.html?prefix=StagedProducts/Hydrography/WBD/HU2/GDB/WBD_19_HU2_GDB.zip) file from the [USGS National Hydrography Dataset](https://prd-tnm.s3.amazonaws.com/index.html?prefix=StagedProducts/Hydrography/WBD/HU2/GDB/) and unzip it into the `r/data/gis/` directory.

#### Configuration

Before running the pipeline, you need to set up the environment variables. These can be set within your shell or using a bash script such as `.env.local.sh` within the `r/` directory.

```sh
# r/.env.local.sh

# database connection
export AKTEMP_HOST=<host>
export AKTEMP_PORT=<port>
export AKTEMP_DBNAME=<dbname>
export AKTEMP_USER=<user>
export AKTEMP_PASSWORD=<password>

# logging
export LOG_LEVEL=DEBUG
export TAR_WARN=FALSE # suppress warnings from targets

# aws credentials
export AWS_REGION=<region>
export AWS_ACCESS_KEY_ID=<access key id>
export AWS_SECRET_ACCESS_KEY=<secret access key>

# s3 bucket for uploading final data files
export AWS_S3_BUCKET=<bucket>
export AWS_S3_PREFIX=<prefix>

# data directory (e.g. data/)
export AKTEMPVIZ_DATA_DIR=<path to data directory>
```

#### Run Pipeline

The pipeline can be run manually or automatically.

To run the pipeline manually, open the `aktempviz.Rproj` file in RStudio and run the following commands:

```r
# set environment variables
Sys.setenv(AKTEMP_HOST = "<host>")
# and so on... see .env.local.sh above

# run the pipeline
source("_targets.R")
targets::tar_make()
```

To run the pipeline automatically, you can use the `run.R` script.

```bash
source .env.local.sh
Rscript run.R
```

### Docker Container

The pipeline can also be run within a Docker container for deployment to the cloud.

1. Build the Docker container:

```bash
cd r
docker build -t aktemp/aktempviz-data --platform linux/amd64 .
```

2. Set up required environment variables in the `.env.docker.local` file:

```ini
# .env.docker.local

AKTEMP_HOST=<host>
AKTEMP_PORT=<port>
AKTEMP_DBNAME=<dbname>
AKTEMP_USER=<user>
AKTEMP_PASSWORD=<password>

LOG_LEVEL=DEBUG
TAR_WARN=FALSE

AWS_REGION=<region>
AWS_ACCESS_KEY_ID=<access key id>
AWS_SECRET_ACCESS_KEY=<secret access key>

AWS_S3_BUCKET=<bucket>
AWS_S3_PREFIX=<prefix>

AKTEMPVIZ_DATA_DIR=/data # important!
```

3. Run the data processing pipeline via docker:

```sh
# run pipeline
docker run \
  --env-file .env.docker \
  -v $(pwd)/data:/data \
  aktemp/aktempviz-data

# open shell in temporary container
docker run -it --rm --entrypoint /bin/bash aktemp/aktempviz-data
```

The pipeline will:

1. Collect data from AKTEMP, USGS, and NPS sources
2. Process and combine the datasets
3. Download daymet tiles
4. Merge water and air temperature data
5. Generate output files in the data directory
6. Upload results to S3

### Cloud Deployment

#### Elastic File System (EFS)

Create an EFS volume for persistent storage of data files (primarily to avoid having to re-download daymet tiles). Apply a security group to allow NFS/2049 access from the internal VPC. Add mount points for the private subnets in each region of the VPC.

```yml
name: aktempviz
fsid: {fsid}
network:
  vpc: {vpc_id}
  security_group:
    - {sg_nfs} (incoming+outgoing NFS/2049 10.0.0.0/16)
  mounts:
    - {region}a, subnet-a (private), {sg_nfs}
    - {region}b, subnet-b (private), {sg_nfs}
```

#### Access EFS in CloudShell

To access the EFS in CloudShell, create a CloudShell VPC environment (Actions > Create VPC Environment)

```yml
name: aktempviz
vpc: {vpc_id}
subnet: {subnet_id} (private)
sg: {sg_id}
```

Set up efs-utils

```sh
sudo yum install -y amazon-efs-utils
```

Mount EFS volume (click Attach button on EFS volume for mount code)

```sh
sudo mkdir /mnt/aktempviz
sudo mount -t efs -o tls {fsid}:/ /mnt/aktempviz
cd /mnt/aktempviz
```

Create initial folders

```sh
sudo mkdir data
sudo mkdir data/_targets
sudo mkdir data/daymet
sudo mkdir data/gis
sudo mkdir data/output
sudo chown -R 1001:1001 data # set owner to 1001
```

#### Upload Files via SFTP

To upload files to EFS via SFTP, create an AWS Transfer server.

```yml
type: SFTP
ip: service managed
endpoint: public
domain: EFS
```

Create IAM policy to read/write to EFS

```yml
name: aktempviz-efs
```

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "RootFileSystemAccess",
            "Effect": "Allow",
            "Action": [
                "elasticfilesystem:ClientRootAccess",
                "elasticfilesystem:ClientMount",
                "elasticfilesystem:ClientWrite"
            ],
            "Resource": "arn:aws:elasticfilesystem:{region}:{account}:file-system/{fsid}"
        }
    ]
}
```

Create IAM role for transfer ([Create an IAM role and policy - AWS Transfer Family](https://docs.aws.amazon.com/transfer/latest/userguide/requirements-roles.html#role-create-procedure))

```yml
name: aktempviz-efs-transfer
service: Transfer
policy: aktempviz-efst
```

Add transfer user

```yml
username: aktempviz
user id: 1001
group id: 1001
role: aktempviz-efs-transfer
home:
  volume: {fsid}
  dir: data
ssh key: id_rsa.pub
```

Connect and upload from trout

```sh
# add server to known_hosts
ssh -i ~/.ssh/id_pub aktempviz@s-{server_id}.server.transfer.{region}.amazonaws.com

# connect to FTP
lftp -u aktempviz sftp://s-{server_id}.server.transfer.{region}.amazonaws.com

# upload daymet
mirror --reverse --verbose --parallel=4 /mnt/data/ecosheds/aktempviz/daymet ./daymet

# upload gis
mirror --reverse --verbose --parallel=4 /mnt/data/ecosheds/aktempviz/gis ./gis

# upload one file
mput hello.txt

# quit
bye
```

#### Deploy Docker Image to ECR

Create new ECR repo

```yml
name: aktemp/aktempviz-data
```

Deploy image to ECR

```sh
aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 083481224424.dkr.ecr.us-west-2.amazonaws.com

docker tag aktemp/aktempviz-data:latest 083481224424.dkr.ecr.us-west-2.amazonaws.com/aktemp/aktempviz-data:latest

docker push 083481224424.dkr.ecr.us-west-2.amazonaws.com/aktemp/aktempviz-data:latest
```

#### Batch Job

AWS Batch can be used to run the pipeline on a schedule.

First, create a compute environment.

```yml
name: aktempviz
type: FARGATE
vcpu: 8 max
vpc: {vpc_id}
subnets:
  - {subnet_id_a} ({region}a, public)
  - {subnet_id_b} ({region}b, public)
sg: {sg_id} (default)
```

Then create a job queue.

```yml
name: aktempviz
type: FARGATE
compute env: aktempviz
```

Create an IAM execution role for the ECS task (job).

```yml
name: aktempviz-execution
service: Elastic Container Service Task
policy: AmazonECSTaskExecutionRolePolicy
```

Create an IAM role for the job that includes S3 access.

```yml
name: aktempviz-job
service: Elastic Container Service Task
policies:
  - AmazonECSTaskExecutionRolePolicy
  - AmazonS3FullAccess
```

Create a job definition that uses the `aktemp/aktempviz-data` Docker image. Add the environment variables and mount point for the EFS volume.

```yml
name: aktempviz-data
type: FARGATE
legacy: FALSE
job attempts: 1
execution role: aktempviz-execution
task role: aktempviz-job
storage volume:
  name: data
  efs: enabled
  filesystem: {fsid}
  root: data/
container:
  name: aktempviz-data
  image: {account}.dkr.ecr.{region}.amazonaws.com/aktemp/aktempviz-data:latest
  cpu: 2
  memory: 4GB
  command: ["Rscript","run.R"]
  env:
    - AKTEMP_DBNAME=postgres
    - AKTEMP_HOST={host}
    - AKTEMP_PORT={port}
    - AKTEMP_USER={user}
    - AKTEMP_PASSWORD={password}
    - LOG_LEVEL=DEBUG
    - TAR_WARN=FALSE
    - AWS_S3_BUCKET={bucket}
    - AWS_S3_PREFIX=viz/data
  mount point:
    volume: data
    path: /data
```

#### Weekly Schedule

To run the pipeline weekly, create an EventBridge rule that triggers a Batch job on Sunday at noon AK.

First, create an EventBridge role:

```yml
name: aktempviz-scheduler
```

The role needs a trust relationship with EventBridge.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "",
            "Effect": "Allow",
            "Principal": {
                "Service": "scheduler.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

Then create an inline policy that allows the job to submit a Batch job.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "batch:SubmitJob"
            ],
            "Resource": "*"
        }
    ]
}
```

Finally, create an EventBridge rule that triggers the Batch job.

```yml
name: aktempviz-data
type: schedule
occurance: recurring
tz: America/Anchorage
cron: 0 12 ? * 1 * (sunday at noon AK)
target: Batch SubmitJob
  job def: aktempviz-data
  job name: aktempviz-data-weekly
  job queue: aktempviz
retry: off
```

## Frontend Application

The frontend application is built with Vue.js 3 and Vuetify and designed to be hosted on AWS S3.

1. Install dependencies:

```bash
npm install
```

2. Start development server:

```bash
npm run dev
```

3. Build and deploy the application:

```bash
npm run build
npm run deploy
```

The data files are not deployed to S3 by this npm script because they are generated using the data pipeline instead.

## License

see [LICENSE](LICENSE)

## Contact

Jeff Walker (@walkerjeffd)  
[Walker Environmental Research, LLC](https://walkerenvres.com)  
<jeff@walkerenvres.com>  
