# AKTEMPVIZ Stream Temperature Data Visualization Tool

https://aktemp.uaa.alaska.edu/viz/

## About

AKTEMPVIZ is a data visualization tool for exploring spatial and temporal water temperature dynamics in streams and rivers across Alaska. The project combines water temperature data from three data sources:

- [AKTEMP Database](https://aktemp.uaa.alaska.edu/) (University of Alaska Anchorage)
- [USGS National Water Information System (NWIS)](https://waterdata.usgs.gov/nwis)
- [National Park Service (NPS) Aquarius Web Portal](https://irma.nps.gov/aqwebportal)

Air temperature data for each water temperature station is obtained from [ERA5-Land](https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_LAND_DAILY_AGGR) via Google Earth Engine.

## Project Structure

- `public/` - Static assets and processed data files
- `public/data/` - Raw and intermediate data files (not tracked in git)
- `R/` - R scripts for data collection and processing
- `src/` - Vue.js frontend application source code

## Data Pipeline

The dataset shown in AKTEMPVIZ is generated by an automated data pipeline using the [{targets}](https://books.ropensci.org/targets/) package for R with cloud storage on AWS S3. The pipeline is run in a Docker container and the results are uploaded to the S3 bucket where the website is hosted.

### Set Up

#### Data Directory

The data directory is where the raw and intermediate data files are stored. The pipeline will write the results to this directory. Note that this directory can be located anywhere on your system, but within the `r/` directory is simplest.

```bash
mkdir r/data
```

Within the `r/data/` directory, you will need to create the following subdirectories:

```bash
mkdir -p r/data/era5 # for ERA5 air temperature cache
mkdir -p r/data/gis # for storing gis data (i.e. WBD)
mkdir -p r/data/output # for storing the pipeline output data
```

**Note**: The targets metadata and objects are now stored in AWS S3 using the targets cloud storage feature, so a local `_targets/` directory is no longer required for persistence.

Within the `r/data/gis/` directory, you will need to download the WBD data. The easiest way to do this is to download the [WBD_19_HU2_GDB](https://prd-tnm.s3.amazonaws.com/index.html?prefix=StagedProducts/Hydrography/WBD/HU2/GDB/WBD_19_HU2_GDB.zip) file from the [USGS National Hydrography Dataset](https://prd-tnm.s3.amazonaws.com/index.html?prefix=StagedProducts/Hydrography/WBD/HU2/GDB/) and unzip it into the `r/data/gis/` directory.

#### Configuration

Before running the pipeline, you need to set up the environment variables. These can be set within your shell or using a bash script such as `.env.local.sh` within the `r/` directory.

```sh
# r/.env.local.sh

# database connection
export AKTEMP_HOST=<host>
export AKTEMP_PORT=<port>
export AKTEMP_DBNAME=<dbname>
export AKTEMP_USER=<user>
export AKTEMP_PASSWORD=<password>

# logging
export LOG_LEVEL=DEBUG
export TAR_WARN=FALSE # suppress warnings from targets

# aws credentials
export AWS_REGION=<region>
export AWS_ACCESS_KEY_ID=<access key id>
export AWS_SECRET_ACCESS_KEY=<secret access key>

# s3 bucket for targets cloud storage and final data files
export AWS_S3_BUCKET=<bucket>
export AWS_S3_PREFIX=<prefix>  # e.g., "viz/data"

# google earth engine service account
# Note: Alternatively, place gee-service-account.json file in r/ directory
export GEE_SERVICE_ACCOUNT_JSON='<full json content>'

# data directory (e.g. data/)
export AKTEMPVIZ_DATA_DIR=<path to data directory>
```

#### Google Earth Engine Setup

The pipeline retrieves air temperature data from ERA5-Land via Google Earth Engine. You'll need a GEE service account:

1. Create a Google Earth Engine service account (see [GEE Service Account docs](https://developers.google.com/earth-engine/guides/service_account))
2. Download the JSON key file
3. Either:
   - Save it as `r/gee-service-account.json`, or
   - Set the `GEE_SERVICE_ACCOUNT_JSON` environment variable with the full JSON content

#### Run Pipeline

The pipeline can be run manually or automatically.

To run the pipeline manually, open the `aktempviz.Rproj` file in RStudio and run the following commands:

```r
# set environment variables
Sys.setenv(AKTEMP_HOST = "<host>")
# and so on... see .env.local.sh above

# run the pipeline
source("_targets.R")
targets::tar_make()
```

To run the pipeline automatically, you can use the `run.R` script.

```bash
source .env.local.sh
Rscript run.R
```

### Docker Container

The pipeline can also be run within a Docker container for deployment to the cloud.

1. Build the Docker container:

```bash
cd r
docker build -t aktemp/aktempviz-data --platform linux/amd64 .
```

2. Set up required environment variables in the `.env.docker.local` file:

```ini
# .env.docker.local

AKTEMP_HOST=<host>
AKTEMP_PORT=<port>
AKTEMP_DBNAME=<dbname>
AKTEMP_USER=<user>
AKTEMP_PASSWORD=<password>

LOG_LEVEL=DEBUG
TAR_WARN=FALSE

AWS_REGION=<region>
AWS_ACCESS_KEY_ID=<access key id>
AWS_SECRET_ACCESS_KEY=<secret access key>

AWS_S3_BUCKET=<bucket>
AWS_S3_PREFIX=<prefix>

AKTEMPVIZ_DATA_DIR=/data # important!
```

3. Run the data processing pipeline via docker:

```sh
# run pipeline
docker run \
  --env-file .env.docker \
  -v $(pwd)/data:/data \
  aktemp/aktempviz-data

# open shell in temporary container
docker run -it --rm --entrypoint /bin/bash aktemp/aktempviz-data
```

The pipeline will:

1. Collect data from AKTEMP, USGS, and NPS sources
2. Process and combine the datasets
3. Fetch ERA5-Land air temperature data from Google Earth Engine
4. Merge water and air temperature data
5. Generate output files in the data directory
6. Upload results to S3

**Note**: The pipeline uses targets cloud storage, so metadata and objects are automatically uploaded to S3 at `s3://{bucket}/{prefix}/meta/` and `s3://{bucket}/{prefix}/objects/`.

### Cloud Deployment

**Note**: EFS is no longer required. The pipeline uses AWS S3 for all persistent storage via the targets cloud storage feature.

#### Deploy Docker Image to ECR

Create new ECR repo

```yml
name: aktemp/aktempviz-data
```

Deploy image to ECR

```sh
aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 083481224424.dkr.ecr.us-west-2.amazonaws.com

docker tag aktemp/aktempviz-data:latest 083481224424.dkr.ecr.us-west-2.amazonaws.com/aktemp/aktempviz-data:latest

docker push 083481224424.dkr.ecr.us-west-2.amazonaws.com/aktemp/aktempviz-data:latest
```

#### Batch Job

AWS Batch can be used to run the pipeline on a schedule.

First, create a compute environment.

```yml
name: aktempviz
type: FARGATE
vcpu: 8 max
vpc: {vpc_id}
subnets:
  - {subnet_id_a} ({region}a, public)
  - {subnet_id_b} ({region}b, public)
sg: {sg_id} (default)
```

Then create a job queue.

```yml
name: aktempviz
type: FARGATE
compute env: aktempviz
```

Create an IAM execution role for the ECS task (job).

```yml
name: aktempviz-execution
service: Elastic Container Service Task
policy: AmazonECSTaskExecutionRolePolicy
```

Create an IAM role for the job that includes S3 access.

```yml
name: aktempviz-job
service: Elastic Container Service Task
policies:
  - AmazonECSTaskExecutionRolePolicy
  - AmazonS3FullAccess
```

Create a job definition that uses the `aktemp/aktempviz-data` Docker image. Add the environment variables including GEE service account credentials.

```yml
name: aktempviz-data
type: FARGATE
legacy: FALSE
job attempts: 1
execution role: aktempviz-execution
task role: aktempviz-job
container:
  name: aktempviz-data
  image: {account}.dkr.ecr.{region}.amazonaws.com/aktemp/aktempviz-data:latest
  cpu: 2
  memory: 4GB
  command: ["Rscript","run.R"]
  env:
    - AKTEMP_DBNAME=postgres
    - AKTEMP_HOST={host}
    - AKTEMP_PORT={port}
    - AKTEMP_USER={user}
    - AKTEMP_PASSWORD={password}
    - LOG_LEVEL=DEBUG
    - TAR_WARN=FALSE
    - AWS_S3_BUCKET={bucket}
    - AWS_S3_PREFIX=viz/data
    - GEE_SERVICE_ACCOUNT_JSON={gee json content}
    - AKTEMPVIZ_DATA_DIR=/data
```

**Note**: No EFS volume mount required. All persistent data is stored in S3.

#### Weekly Schedule

To run the pipeline weekly, create an EventBridge rule that triggers a Batch job on Sunday at noon AK.

First, create an EventBridge role:

```yml
name: aktempviz-scheduler
```

The role needs a trust relationship with EventBridge.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "",
            "Effect": "Allow",
            "Principal": {
                "Service": "scheduler.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

Then create an inline policy that allows the job to submit a Batch job.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "batch:SubmitJob"
            ],
            "Resource": "*"
        }
    ]
}
```

Finally, create an EventBridge rule that triggers the Batch job.

```yml
name: aktempviz-data
type: schedule
occurance: recurring
tz: America/Anchorage
cron: 0 12 ? * 1 * (sunday at noon AK)
target: Batch SubmitJob
  job def: aktempviz-data
  job name: aktempviz-data-weekly
  job queue: aktempviz
retry: off
```

## Frontend Application

The frontend application is built with Vue.js 3 and Vuetify and designed to be hosted on AWS S3.

1. Install dependencies:

```bash
npm install
```

2. Start development server:

```bash
npm run dev
```

3. Build and deploy the application:

```bash
npm run build
npm run deploy
```

The data files are not deployed to S3 by this npm script because they are generated using the data pipeline instead.

## License

see [LICENSE](LICENSE)

## Contact

Jeff Walker (@walkerjeffd)  
[Walker Environmental Research, LLC](https://walkerenvres.com)  
<jeff@walkerenvres.com>  
